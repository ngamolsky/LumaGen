{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_STRING = \"\"\"\n",
    "This is AI News! an MVP of a service that goes thru all AI discords/Twitters/reddits and summarizes what people are talking about, so that you can keep up without the fatigue. Signing up here opts you in to the real thing when we launch it ðŸ”œ\n",
    "\n",
    "a quiet day is all you need.\n",
    "\n",
    "AI News for 9/25/2024-9/26/2024. We checked 7 subreddits, 433 Twitters and 31 Discords (224 channels, and 3282 messages) for you. Estimated reading time saved (at 200wpm): 342 minutes. You can now tag @smol_ai for AINews discussions!\n",
    "\n",
    "Many are still processing the surprise management turnover at OpenAI. Sama and gdb both posted statements. It seems the Anthropic rumors were postponed, but in the meantime the new blueberry model rumor mill is just getting started.\n",
    "\n",
    "Since it's a quiet day, you could help out AINews by checking out the RAG++ course from Weights and Biases! We featured it yesterday but forgot to include the text link. Sorry!\n",
    "\n",
    "Swyx: Something we also missed in our initial scan yesterday was chapters 6 and 7 on response synthesis and optimmization. Chapter 6 in particular is exactly what we had to do to build AINews - everything you see below is AI generated thanks to these techniques.\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORYBOARD_PROMPT = \"\"\"\n",
    "You are a helpful assistant that takes a source string of news and exciting events, and returns a storyboard for a short video with content about the news. \n",
    "\n",
    "Each storyboard item should contain a description of a starting image (which we will generate based on your description), a description of the ending image (which we will also generate based on your description), and a description of the action that should happen in between\n",
    "\n",
    "SOURCE STRING:\n",
    "{source_string}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Prepare the prompt\n",
    "prompt = STORYBOARD_PROMPT.format(source_string=SOURCE_STRING)\n",
    "\n",
    "\n",
    "class Storyboard(BaseModel):\n",
    "    start_image_description: str\n",
    "    end_image_description: str\n",
    "    action: str\n",
    "\n",
    "# Call OpenAI API with GPT-4 Turbo\n",
    "response = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "    ],\n",
    "    response_format=Storyboard\n",
    ")\n",
    "\n",
    "# Extract the generated storyboard\n",
    "storyboard = response.choices[0].message.parsed\n",
    "\n",
    "print(\"Generated Storyboard:\")\n",
    "print(storyboard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import requests\n",
    "IDEOGRAM_URL = \"https://api.ideogram.ai/generate\"\n",
    "\n",
    "IDEOGRAM_HEADERS = {\n",
    "    \"Api-Key\": os.getenv(\"IDEOGRAM_API_KEY\"),\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "start_image_request = {\n",
    "    \"image_request\": {\n",
    "        \"prompt\": storyboard.start_image_description,\n",
    "        \"model\": \"V_2\",\n",
    "        \"magic_prompt_option\": \"AUTO\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "start_image_response = requests.post(IDEOGRAM_URL, json=start_image_request, headers=IDEOGRAM_HEADERS)\n",
    "print(start_image_response.json())\n",
    "\n",
    "end_image_request = {\n",
    "    \"image_request\": {\n",
    "        \"prompt\": storyboard.end_image_description,\n",
    "        \"model\": \"V_2\",\n",
    "        \"magic_prompt_option\": \"AUTO\"\n",
    "    }   \n",
    "}\n",
    "\n",
    "end_image_response = requests.post(IDEOGRAM_URL, json=end_image_request, headers=IDEOGRAM_HEADERS)\n",
    "print(end_image_response.json())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lumaai import LumaAI\n",
    "\n",
    "client = LumaAI()\n",
    "\n",
    "generation = client.generations.create(\n",
    "    prompt=storyboard.action,\n",
    "    keyframes={\n",
    "      \"frame0\": {\n",
    "        \"type\": \"image\",\n",
    "        \"url\": start_image_response.json()[\"data\"][0][\"url\"]\n",
    "      },\n",
    "      \"frame1\": {\n",
    "        \"type\": \"image\",\n",
    "        \"url\": end_image_response.json()[\"data\"][0][\"url\"]\n",
    "      }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(generation.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def poll_generation(generation_id, max_attempts=30, delay=10):\n",
    "    for attempt in range(max_attempts):\n",
    "        status = client.generations.get(generation_id)\n",
    "        if status.state == \"completed\":\n",
    "            return status\n",
    "        elif status.state == \"failed\":\n",
    "            raise Exception(f\"Generation failed: {status.failure_reason}\")\n",
    "        print(f\"Generation in progress... (Attempt {attempt + 1}/{max_attempts})\")\n",
    "        time.sleep(delay)\n",
    "    raise Exception(\"Generation timed out\")\n",
    "\n",
    "try:\n",
    "    result = poll_generation(generation.id)\n",
    "    print(\"Generation completed successfully!\")\n",
    "    print(f\"Result URL: {result.assets}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "luma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
